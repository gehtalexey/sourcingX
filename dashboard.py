"""
LinkedIn Profile Enricher Dashboard
Run with: streamlit run dashboard.py
"""

import streamlit as st
import pandas as pd
import json
import time
import re
import requests
import winsound
from pathlib import Path
from datetime import datetime
from plyer import notification
from openai import OpenAI
import gspread
from google.oauth2.service_account import Credentials
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


# Page config
st.set_page_config(
    page_title="LinkedIn Enricher",
    page_icon="ðŸ”",
    layout="wide"
)

# Load API keys
def load_config():
    config_path = Path(__file__).parent / 'config.json'
    if config_path.exists():
        with open(config_path, 'r') as f:
            return json.load(f)
    return {}

def load_api_key():
    config = load_config()
    return config.get('api_key')

def load_openai_key():
    config = load_config()
    return config.get('openai_api_key')


def load_phantombuster_key():
    config = load_config()
    return config.get('phantombuster_api_key')


def load_phantombuster_agent_id():
    config = load_config()
    return config.get('phantombuster_agent_id')


@st.cache_data(ttl=300)  # Cache for 5 minutes
def fetch_phantombuster_agents(api_key: str) -> list[dict]:
    """Fetch list of all PhantomBuster agents."""
    try:
        response = requests.get(
            'https://api.phantombuster.com/api/v2/agents/fetch-all',
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )
        if response.status_code == 200:
            agents = response.json()
            # Return list of {id, name} for dropdown
            return [{'id': a['id'], 'name': a['name']} for a in agents]
        return []
    except Exception as e:
        return []


def fetch_phantombuster_results(api_key: str, agent_id: str) -> list[dict]:
    """Fetch results from PhantomBuster agent."""
    try:
        # Get agent output
        response = requests.get(
            f'https://api.phantombuster.com/api/v2/agents/fetch-output',
            params={'id': agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=60
        )

        if response.status_code == 200:
            data = response.json()
            # PhantomBuster returns results in 'output' or as direct JSON
            if 'output' in data:
                # Parse the output if it's a JSON string
                output = data['output']
                if isinstance(output, str):
                    try:
                        return json.loads(output)
                    except:
                        return []
                return output if isinstance(output, list) else []
            return []
        else:
            return []
    except Exception as e:
        st.error(f"PhantomBuster API error: {e}")
        return []


def fetch_phantombuster_result_csv(api_key: str, agent_id: str, debug: bool = False, filename: str = None) -> pd.DataFrame:
    """Fetch results from PhantomBuster agent. Tries multiple methods:
    1. Authenticated API to get output files
    2. Result object from container

    Args:
        filename: Optional specific filename to fetch (without .csv extension)
    """
    from io import StringIO

    try:
        # First, try to get the agent's info
        agent_response = requests.get(
            'https://api.phantombuster.com/api/v2/agents/fetch',
            params={'id': agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )

        agent_name = 'Unknown'
        s3_folder = None
        org_s3_folder = None

        if agent_response.status_code == 200:
            agent_data = agent_response.json()
            s3_folder = agent_data.get('s3Folder')
            org_s3_folder = agent_data.get('orgS3Folder')
            agent_name = agent_data.get('name', 'Unknown')

            if debug:
                st.info(f"Agent: {agent_name}")
                st.info(f"S3 Folder: {s3_folder}")
                if filename:
                    st.info(f"Looking for file: {filename}.csv")
                with st.expander("Agent API Response"):
                    st.json(agent_data)

        # Method 0: Try direct PhantomBuster cache URL (most reliable for files)
        if s3_folder and org_s3_folder:
            # If specific filename provided, try it first
            files_to_try = []
            if filename:
                files_to_try = [f'{filename}.csv', f'{filename}.json']
            else:
                files_to_try = ['result.csv', 'result.json']

            for fname in files_to_try:
                try:
                    cache_url = f'https://cache1.phantombooster.com/{org_s3_folder}/{s3_folder}/{fname}'
                    if debug:
                        st.info(f"Trying cache URL: {cache_url}")
                    cache_response = requests.get(cache_url, timeout=60)
                    if cache_response.status_code == 200:
                        if fname.endswith('.csv'):
                            df = pd.read_csv(StringIO(cache_response.text))
                            if not df.empty:
                                if debug:
                                    st.success(f"Loaded from cache URL: {fname}")
                                return df
                        elif fname.endswith('.json'):
                            profiles = cache_response.json()
                            if isinstance(profiles, list) and profiles:
                                if debug:
                                    st.success(f"Loaded from cache URL: {fname}")
                                return pd.DataFrame(profiles)
                except Exception as e:
                    if debug:
                        st.warning(f"Cache URL error: {e}")

        # Method 1: Try fetch-output endpoint (gets last run output/logs)
        try:
            output_response = requests.get(
                'https://api.phantombuster.com/api/v2/agents/fetch-output',
                params={'id': agent_id},
                headers={'X-Phantombuster-Key': api_key},
                timeout=60
            )

            if debug:
                st.info(f"Fetch-output API: {output_response.status_code}")

            if output_response.status_code == 200:
                output_data = output_response.json()
                if debug:
                    with st.expander("Fetch-output Response"):
                        # Truncate output for display
                        display_data = output_data.copy()
                        if 'output' in display_data and len(str(display_data['output'])) > 2000:
                            display_data['output'] = str(display_data['output'])[:2000] + '... (truncated)'
                        st.json(display_data)

                # Check for resultObject in output
                if output_data.get('resultObject'):
                    result_obj = output_data['resultObject']
                    if isinstance(result_obj, str):
                        profiles = json.loads(result_obj)
                    else:
                        profiles = result_obj
                    if isinstance(profiles, list) and profiles:
                        st.success(f"Loaded {len(profiles)} profiles from fetch-output")
                        return pd.DataFrame(profiles)
        except Exception as e:
            if debug:
                st.warning(f"Fetch-output error: {e}")

        # Method 2: Try the store/fetch API endpoint (authenticated file access)
        # First, list all files in the agent's storage
        try:
            files_response = requests.get(
                'https://api.phantombuster.com/api/v2/agents/fetch-output',
                params={'id': agent_id},
                headers={'X-Phantombuster-Key': api_key},
                timeout=30
            )
            if files_response.status_code == 200:
                files_data = files_response.json()
                if debug:
                    with st.expander("Agent Output Data"):
                        st.json({k: v for k, v in files_data.items() if k != 'output'})
        except:
            pass

        # Try common file patterns - Sales Navigator Export uses these
        possible_files = []

        # If specific filename provided, ONLY try that file (strict mode)
        if filename:
            possible_files = [f'{filename}.csv', f'{filename}.json']
        else:
            # Add agent name-based files (Sales Navigator Export pattern)
            if agent_name and agent_name != 'Unknown':
                possible_files.append(f'{agent_name}.csv')
                possible_files.append(f'{agent_name}.json')

            # Common PhantomBuster output files - Sales Navigator Export uses database files
            # Try many variations of database filename patterns
            possible_files.extend([
                'result.csv',
                'result.json',
                'database-result.csv',
                'database-linkedin-sales-navigator-search-export.csv',
                'database-Sales Navigator Search Export.csv',
                'Sales Navigator Search Export.csv',
                'database-sales-navigator-search-export.csv',
                'LinkedIn Sales Navigator Search Export result.csv',
                'Sales Navigator Search Export result.csv',
                # Lowercase variations
                'database-sales navigator search export.csv',
                'sales navigator search export.csv',
                # With underscores
                'database_linkedin_sales_navigator_search_export.csv',
                'database_result.csv',
                # Output variations
                'output.csv',
                'output.json',
                'profiles.csv',
                'leads.csv',
            ])

            # Also try with s3Folder prefix patterns
            if s3_folder:
                possible_files.insert(0, f'database-linkedin-sales-navigator-search-export.csv')
                possible_files.insert(0, f'result.csv')

        # Try to list all files in agent storage using store API
        try:
            # Try listing files with common patterns
            for pattern in ['database-', 'result', agent_name.replace(' ', '-').lower() if agent_name else '']:
                if not pattern:
                    continue
                list_response = requests.get(
                    'https://api.phantombuster.com/api/v2/store/fetch',
                    params={'id': agent_id, 'name': f'{pattern}'},
                    headers={'X-Phantombuster-Key': api_key},
                    timeout=10
                )
                # 200 means file exists, add it
                if list_response.status_code == 200:
                    if debug:
                        st.success(f"Found file with pattern: {pattern}")
        except Exception as e:
            if debug:
                st.warning(f"List files error: {e}")

        if debug:
            st.info(f"Trying files: {possible_files[:5]}...")

        for fname in possible_files:
            try:
                # Try with agent ID
                store_response = requests.get(
                    'https://api.phantombuster.com/api/v2/store/fetch',
                    params={'id': agent_id, 'name': fname},
                    headers={'X-Phantombuster-Key': api_key},
                    timeout=60
                )

                if debug:
                    st.info(f"Store API {fname}: {store_response.status_code}")

                # If 404, try with s3Folder path
                if store_response.status_code == 404 and s3_folder:
                    store_response = requests.get(
                        'https://api.phantombuster.com/api/v2/store/fetch',
                        params={'id': agent_id, 'name': f'{s3_folder}/{fname}'},
                        headers={'X-Phantombuster-Key': api_key},
                        timeout=60
                    )
                    if debug:
                        st.info(f"Store API (with folder) {s3_folder}/{fname}: {store_response.status_code}")

                # Also try direct org folder access
                if store_response.status_code == 404 and org_s3_folder:
                    store_response = requests.get(
                        'https://api.phantombuster.com/api/v2/store/fetch',
                        params={'id': agent_id, 'name': f'{org_s3_folder}/{s3_folder}/{fname}'},
                        headers={'X-Phantombuster-Key': api_key},
                        timeout=60
                    )
                    if debug:
                        st.info(f"Store API (full path): {store_response.status_code}")

                if store_response.status_code == 200:
                    content_type = store_response.headers.get('content-type', '')

                    if 'csv' in fname or 'csv' in content_type:
                        df = pd.read_csv(StringIO(store_response.text))
                        if not df.empty:
                            st.success(f"Loaded {len(df)} profiles from {fname}")
                            return df
                    elif 'json' in fname or 'json' in content_type:
                        profiles = store_response.json()
                        if isinstance(profiles, list) and profiles:
                            st.success(f"Loaded {len(profiles)} profiles from {fname}")
                            return pd.DataFrame(profiles)
            except Exception as e:
                if debug:
                    st.warning(f"Store API {fname}: {e}")


        # Try the container result object method
        response = requests.get(
            'https://api.phantombuster.com/api/v2/containers/fetch-all',
            params={'agentId': agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=60
        )

        if response.status_code != 200:
            st.error(f"Failed to fetch containers: {response.status_code}")
            return pd.DataFrame()

        data = response.json()
        containers = data.get('containers', [])

        if debug:
            st.info(f"Found {len(containers)} containers")

        if not containers:
            st.error("No runs found for this agent. Run the agent first.")
            return pd.DataFrame()

        # Get the most recent finished container
        finished_containers = [c for c in containers if c.get('status') == 'finished']
        if not finished_containers:
            st.error("No completed runs found. Wait for the agent to finish.")
            return pd.DataFrame()

        container = finished_containers[0]
        container_id = container['id']

        if debug:
            with st.expander("Latest Container"):
                st.json(container)

        # Check if container has output file URL
        output_url = container.get('resultObject') or container.get('output')
        if output_url and isinstance(output_url, str) and output_url.startswith('http'):
            try:
                output_response = requests.get(output_url, timeout=60)
                if output_response.status_code == 200:
                    if '.csv' in output_url or 'text/csv' in output_response.headers.get('content-type', ''):
                        df = pd.read_csv(StringIO(output_response.text))
                        if not df.empty:
                            st.success(f"Loaded {len(df)} profiles from container output")
                            return df
                    else:
                        profiles = output_response.json()
                        if isinstance(profiles, list) and profiles:
                            return pd.DataFrame(profiles)
            except Exception as e:
                if debug:
                    st.warning(f"Container output URL failed: {e}")

        # Fetch the result object using the container ID
        result_response = requests.get(
            'https://api.phantombuster.com/api/v2/containers/fetch-result-object',
            params={'id': container_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=60
        )

        if debug:
            with st.expander("Result Object Response"):
                st.json(result_response.json() if result_response.status_code == 200 else {"status": result_response.status_code})

        if result_response.status_code != 200:
            st.error(f"Failed to fetch results: {result_response.status_code}")
            return pd.DataFrame()

        result_data = result_response.json()
        result_object = result_data.get('resultObject')

        if not result_object:
            # Check if the output says "already been processed"
            output_text = ""
            try:
                out_resp = requests.get(
                    'https://api.phantombuster.com/api/v2/agents/fetch-output',
                    params={'id': agent_id},
                    headers={'X-Phantombuster-Key': api_key},
                    timeout=30
                )
                if out_resp.status_code == 200:
                    output_text = out_resp.json().get('output', '')
            except:
                pass

            if 'already been processed' in output_text:
                st.warning("""
**No new profiles found.**

The phantom says "This search has already been processed" - meaning it already scraped these profiles before.

**To get fresh results:**
1. Go to **Launch Search** section below
2. Enter your name and a NEW Sales Navigator search URL
3. Click **Launch** - this will clear old results and start fresh
                """)
            else:
                st.error("No result object found.")
                st.warning(f"""
**Troubleshooting:**
- Agent: {agent_name}
- Containers found: {len(containers)}
- Finished runs: {len(finished_containers)}
- Latest run status: {container.get('status')}

The phantom may not have produced any results. Try:
1. Running the phantom with a new search URL
2. Or download CSV directly from PhantomBuster dashboard
                """)
            return pd.DataFrame()

        # Parse the result object (it's a JSON string)
        if isinstance(result_object, str):
            profiles = json.loads(result_object)
        else:
            profiles = result_object

        if isinstance(profiles, list):
            return pd.DataFrame(profiles)
        else:
            return pd.DataFrame([profiles])

    except json.JSONDecodeError as e:
        st.error(f"Error parsing PhantomBuster results: {e}")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"PhantomBuster fetch error: {e}")
        return pd.DataFrame()


def find_user_phantom(api_key: str, user_email: str, agents: list) -> dict:
    """Find a phantom that belongs to a specific user based on email in the name.

    Returns the agent dict if found, None otherwise.
    """
    user_identifier = user_email.split('@')[0].lower()  # Use part before @ for matching

    for agent in agents:
        agent_name = agent.get('name', '').lower()
        # Check if agent name contains the user identifier
        if user_identifier in agent_name or user_email.lower() in agent_name:
            return agent

    return None


def duplicate_phantom_for_user(api_key: str, template_agent_id: str, user_email: str) -> dict:
    """Duplicate a template phantom for a new user.

    Returns dict with 'id' and 'name' on success, or 'error' on failure.
    """
    try:
        # First fetch the template agent's full config
        fetch_response = requests.get(
            'https://api.phantombuster.com/api/v2/agents/fetch',
            params={'id': template_agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )

        if fetch_response.status_code != 200:
            return {'error': f"Failed to fetch template: {fetch_response.status_code}"}

        template = fetch_response.json()

        # Create new agent name with user email
        user_name = user_email.split('@')[0].title()
        new_name = f"Sales Nav Export - {user_name}"

        # Prepare the new agent config (copy from template)
        new_agent = {
            'name': new_name,
            'script': template.get('script'),
            'branch': template.get('branch', 'master'),
            'environment': template.get('environment', 'release'),
            'argument': template.get('argument', '{}'),
        }

        # Create the new agent
        create_response = requests.post(
            'https://api.phantombuster.com/api/v2/agents/save',
            headers={
                'X-Phantombuster-Key': api_key,
                'Content-Type': 'application/json'
            },
            json=new_agent,
            timeout=30
        )

        if create_response.status_code == 200:
            result = create_response.json()
            return {'id': result.get('id'), 'name': new_name}
        else:
            return {'error': f"Failed to create: {create_response.status_code} - {create_response.text}"}

    except Exception as e:
        return {'error': str(e)}


def update_phantombuster_search_url(api_key: str, agent_id: str, search_url: str, num_profiles: int = 2500, csv_name: str = None) -> dict:
    """Update the phantom's saved search URL configuration.

    This updates the phantom's saved argument so it uses the new search URL
    when launched with saved settings.

    Args:
        api_key: PhantomBuster API key
        agent_id: Agent ID
        search_url: Sales Navigator search URL
        num_profiles: Number of profiles to scrape
        csv_name: Custom output filename (without .csv extension). If None, uses timestamp.

    Returns dict with 'success': True, 'csvName': filename or 'error': message
    """
    from datetime import datetime

    try:
        # First fetch current agent config to preserve other settings
        fetch_response = requests.get(
            'https://api.phantombuster.com/api/v2/agents/fetch',
            params={'id': agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )

        if fetch_response.status_code != 200:
            return {'error': f"Failed to fetch agent: {fetch_response.status_code}"}

        agent_data = fetch_response.json()
        current_argument = agent_data.get('argument', '{}')

        # Parse current argument
        try:
            if isinstance(current_argument, str):
                arg_dict = json.loads(current_argument)
            else:
                arg_dict = current_argument
        except:
            arg_dict = {}

        # Generate timestamped filename if not provided
        if csv_name is None:
            timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')
            csv_name = f'search_{timestamp}'

        # Update search URL, profile count, and output filename
        arg_dict['salesNavigatorSearchUrl'] = search_url
        arg_dict['search'] = search_url
        arg_dict['numberOfProfiles'] = num_profiles
        arg_dict['numberOfResultsPerSearch'] = num_profiles
        arg_dict['csvName'] = csv_name  # PhantomBuster uses this for output file naming

        # Update the agent with new argument
        update_response = requests.post(
            'https://api.phantombuster.com/api/v2/agents/save',
            headers={
                'X-Phantombuster-Key': api_key,
                'Content-Type': 'application/json'
            },
            json={
                'id': agent_id,
                'argument': json.dumps(arg_dict)
            },
            timeout=30
        )

        if update_response.status_code == 200:
            return {'success': True, 'csvName': csv_name}
        else:
            return {'error': f"Failed to update: {update_response.status_code} - {update_response.text}"}

    except Exception as e:
        return {'error': str(e)}


def list_phantombuster_files(api_key: str, agent_id: str) -> list[dict]:
    """List all files in a PhantomBuster agent's storage.

    Returns list of dicts with 'name', 'size', 'lastModified' for each file.
    """
    try:
        # First get agent info to get S3 folder
        agent_response = requests.get(
            'https://api.phantombuster.com/api/v2/agents/fetch',
            params={'id': agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )

        if agent_response.status_code != 200:
            return []

        agent_data = agent_response.json()
        s3_folder = agent_data.get('s3Folder')
        org_s3_folder = agent_data.get('orgS3Folder')

        files = []

        # Try to list files from agent's fileMgmt if available
        file_mgmt = agent_data.get('fileMgmt', {})
        if file_mgmt:
            for filename, info in file_mgmt.items():
                files.append({
                    'name': filename,
                    'size': info.get('size', 0),
                    'lastModified': info.get('lastModified', ''),
                })

        # Also try common file patterns via store API
        common_files = [
            'result.csv', 'result.json',
            'database-linkedin-sales-navigator-search-export.csv',
            'database-sales-navigator-search-export.csv',
        ]

        # Add timestamped patterns (recent dates)
        from datetime import datetime, timedelta
        for i in range(30):  # Check last 30 days
            date = datetime.now() - timedelta(days=i)
            date_str = date.strftime('%Y-%m-%d')
            common_files.append(f'result_{date_str}.csv')
            common_files.append(f'search_{date_str}.csv')

        for fname in common_files:
            try:
                check_response = requests.head(
                    'https://api.phantombuster.com/api/v2/store/fetch',
                    params={'id': agent_id, 'name': fname},
                    headers={'X-Phantombuster-Key': api_key},
                    timeout=5
                )
                if check_response.status_code == 200:
                    # File exists, add if not already in list
                    if not any(f['name'] == fname for f in files):
                        files.append({
                            'name': fname,
                            'size': int(check_response.headers.get('content-length', 0)),
                            'lastModified': check_response.headers.get('last-modified', ''),
                        })
            except:
                pass

        # Try cache URL listing if we have S3 info
        if s3_folder and org_s3_folder:
            try:
                cache_base = f'https://cache1.phantombooster.com/{org_s3_folder}/{s3_folder}/'
                # Check for result files
                for fname in ['result.csv', 'result.json']:
                    try:
                        check = requests.head(cache_base + fname, timeout=5)
                        if check.status_code == 200:
                            if not any(f['name'] == fname for f in files):
                                files.append({
                                    'name': fname,
                                    'size': int(check.headers.get('content-length', 0)),
                                    'lastModified': check.headers.get('last-modified', ''),
                                })
                    except:
                        pass
            except:
                pass

        return files
    except Exception as e:
        return []


def delete_phantombuster_file(api_key: str, agent_id: str, filename: str) -> bool:
    """Delete a file from PhantomBuster agent's storage.

    Returns True if deleted successfully or file didn't exist.
    """
    try:
        response = requests.delete(
            'https://api.phantombuster.com/api/v2/store/delete',
            params={'id': agent_id, 'name': filename},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )
        # 200 = deleted, 404 = didn't exist (both are fine)
        return response.status_code in [200, 204, 404]
    except Exception:
        return False


def clear_all_phantombuster_files(api_key: str, agent_id: str) -> int:
    """Delete ALL files from PhantomBuster agent's storage for a completely fresh start.

    Returns the number of files deleted.
    """
    deleted_count = 0
    try:
        # First, get the agent info to find storage folders
        response = requests.get(
            'https://api.phantombuster.com/api/v2/agents/fetch',
            params={'id': agent_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )

        if response.status_code == 200:
            agent_data = response.json()
            s3_folder = agent_data.get('s3Folder', '')
            org_s3_folder = agent_data.get('orgS3Folder', '')

            if s3_folder and org_s3_folder:
                # Try to list files from PhantomBuster storage
                # Use the store/fetch-all endpoint if available, otherwise delete known patterns
                files_response = requests.get(
                    f'https://cache1.phantombooster.com/{org_s3_folder}/{s3_folder}/',
                    timeout=30
                )

                # Try common file patterns
                common_files = [
                    'result.csv', 'result.json',
                    'database-result.csv', 'database.csv',
                    'database-linkedin-sales-navigator-search-export.csv',
                    'database-Sales Navigator Search Export.csv',
                    'database-sales-navigator-search-export.csv',
                ]

                for filename in common_files:
                    if delete_phantombuster_file(api_key, agent_id, filename):
                        deleted_count += 1

    except Exception:
        pass

    return deleted_count


def launch_phantombuster_agent(api_key: str, agent_id: str, argument: dict = None, clear_results: bool = False) -> dict:
    """Launch a PhantomBuster agent with the given argument.

    Returns dict with 'containerId' on success, or 'error' on failure.
    Note: Passing any argument overrides the phantom's saved config including cookie!

    Args:
        clear_results: If True, delete existing result AND database files before launching for fresh results
    """
    try:
        # Delete existing results AND database for a fresh start
        if clear_results:
            # Delete result files
            delete_phantombuster_file(api_key, agent_id, 'result.csv')
            delete_phantombuster_file(api_key, agent_id, 'result.json')
            # Delete ALL possible database files - the phantom stores accumulated profiles here
            database_files = [
                'database-result.csv',
                'database-linkedin-sales-navigator-search-export.csv',
                'database-Sales Navigator Search Export.csv',
                'database-sales-navigator-search-export.csv',
                'database-sales navigator search export.csv',
                'database_linkedin_sales_navigator_search_export.csv',
                'database_result.csv',
                'database.csv',
            ]
            for db_file in database_files:
                delete_phantombuster_file(api_key, agent_id, db_file)

        payload = {'id': agent_id}
        if argument:
            # Pass argument as JSON string to merge with saved config rather than replace
            payload['argument'] = json.dumps(argument)

        response = requests.post(
            'https://api.phantombuster.com/api/v2/agents/launch',
            headers={
                'X-Phantombuster-Key': api_key,
                'Content-Type': 'application/json'
            },
            json=payload,
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            return {'containerId': data.get('containerId')}
        else:
            return {'error': f"API error {response.status_code}: {response.text}"}
    except Exception as e:
        return {'error': str(e)}


def fetch_container_status(api_key: str, container_id: str) -> dict:
    """Fetch the status of a PhantomBuster container.

    Returns dict with 'status' (running, finished, error) and other details.
    """
    try:
        response = requests.get(
            'https://api.phantombuster.com/api/v2/containers/fetch',
            params={'id': container_id},
            headers={'X-Phantombuster-Key': api_key},
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            result = {
                'status': data.get('status', 'unknown'),
                'exitCode': data.get('exitCode'),
                'exitMessage': data.get('exitMessage'),
                'progress': data.get('progress'),
                'executionTime': data.get('executionTime'),
                'output': data.get('output', ''),
            }

            # Try to extract profile count from output (PhantomBuster logs progress)
            output = data.get('output', '')
            if output:
                # Look for patterns like "Scraped 50 profiles" or "50 profiles saved"
                import re
                matches = re.findall(r'(\d+)\s*(?:profiles?|leads?|results?)', output.lower())
                if matches:
                    result['profiles_count'] = int(matches[-1])  # Get last match

                # Look for progress percentage
                pct_matches = re.findall(r'(\d+)%', output)
                if pct_matches:
                    result['progress_pct'] = int(pct_matches[-1])

            return result
        else:
            return {'status': 'error', 'error': f"API error {response.status_code}"}
    except Exception as e:
        return {'status': 'error', 'error': str(e)}


def normalize_phantombuster_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize PhantomBuster column names to match expected dashboard format."""
    # Column mapping: PhantomBuster name -> dashboard expected name
    # Order matters - first match wins for columns mapping to same target
    column_map = {
        'firstName': 'first_name',
        'lastName': 'last_name',
        'fullName': 'full_name',
        'title': 'headline',
        'headline': 'headline',
        'companyName': 'current_company',
        'company': 'current_company',
        'currentCompanyName': 'current_company',
        'location': 'location',
        'linkedInProfileUrl': 'public_url',
        'profileUrl': 'public_url',
        'linkedinUrl': 'public_url',
        'defaultProfileUrl': 'public_url',
        'vmid': 'vmid',
        'connectionDegree': 'connection_degree',
        'mutualConnectionsCount': 'mutual_connections',
        'premium': 'is_premium',
        'openLink': 'is_open_link',
        'jobTitle': 'current_title',
        'currentJobTitle': 'current_title',
        'summary': 'summary',
        'query': 'search_query',
        'timestamp': 'scraped_at',
    }

    # Rename columns - track which target names are already used to avoid duplicates
    rename_dict = {}
    used_targets = set(df.columns)  # Start with existing column names

    for old_name, new_name in column_map.items():
        if old_name in df.columns and new_name not in used_targets:
            rename_dict[old_name] = new_name
            used_targets.add(new_name)

    df = df.rename(columns=rename_dict)

    # Create first_name/last_name from fullName if needed
    if 'full_name' in df.columns and 'first_name' not in df.columns:
        df['first_name'] = df['full_name'].apply(lambda x: str(x).split()[0] if pd.notna(x) and str(x).strip() else '')
        df['last_name'] = df['full_name'].apply(lambda x: ' '.join(str(x).split()[1:]) if pd.notna(x) and len(str(x).split()) > 1 else '')

    # Use headline as current_title if current_title doesn't exist
    if 'headline' in df.columns and 'current_title' not in df.columns:
        df['current_title'] = df['headline']

    return df


def extract_urls_from_phantombuster(df: pd.DataFrame) -> list[str]:
    """Extract LinkedIn URLs from PhantomBuster results."""
    urls = []
    # Try different column names PhantomBuster might use
    url_columns = ['linkedInProfileUrl', 'defaultProfileUrl', 'url', 'profileUrl', 'LinkedIn Sales Navigator profile URL']

    for col in url_columns:
        if col in df.columns:
            urls.extend(df[col].dropna().tolist())
            break

    # Normalize URLs
    normalized = []
    for u in urls:
        if u and 'linkedin.com' in str(u):
            u = str(u).strip()
            if not u.startswith('http'):
                u = 'https://' + u
            normalized.append(u)
    return normalized


def get_gspread_client():
    """Get authenticated gspread client using service account."""
    config = load_config()
    creds_file = config.get('google_credentials_file')
    if not creds_file:
        return None

    creds_path = Path(__file__).parent / creds_file
    if not creds_path.exists():
        return None

    scopes = [
        'https://www.googleapis.com/auth/spreadsheets.readonly',
        'https://www.googleapis.com/auth/drive.readonly'
    ]

    creds = Credentials.from_service_account_file(str(creds_path), scopes=scopes)
    return gspread.authorize(creds)


def load_sheet_as_df(sheet_url: str, worksheet_name: str = None) -> pd.DataFrame:
    """Load a Google Sheet as a pandas DataFrame."""
    client = get_gspread_client()
    if not client:
        return None

    try:
        spreadsheet = client.open_by_url(sheet_url)
        if worksheet_name:
            worksheet = spreadsheet.worksheet(worksheet_name)
        else:
            worksheet = spreadsheet.sheet1

        data = worksheet.get_all_records()
        return pd.DataFrame(data)
    except Exception as e:
        st.error(f"Error loading sheet: {e}")
        return None


def get_filter_sheets_config():
    """Get filter sheets URLs from config."""
    config = load_config()
    return config.get('filter_sheets', {})


def send_notification(title, message):
    """Send desktop notification with sound."""
    try:
        winsound.PlaySound("SystemExclamation", winsound.SND_ALIAS)
        notification.notify(
            title=title,
            message=message,
            app_name="LinkedIn Enricher",
            timeout=10
        )
    except Exception:
        try:
            winsound.MessageBeep(winsound.MB_ICONEXCLAMATION)
        except:
            pass


def extract_urls(uploaded_file) -> list[str]:
    """Extract LinkedIn URLs from uploaded file."""
    urls = []

    if uploaded_file.name.endswith('.json'):
        data = json.load(uploaded_file)
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    url = (item.get('url') or item.get('linkedin_url') or
                           item.get('profile_url') or item.get('linkedinUrl') or
                           item.get('public_url'))
                    if url:
                        urls.append(url)
                elif isinstance(item, str):
                    urls.append(item)

    elif uploaded_file.name.endswith('.csv'):
        df = pd.read_csv(uploaded_file)
        for col in ['url', 'linkedin_url', 'profile_url', 'URL', 'LinkedIn URL', 'linkedinUrl', 'LinkedIn', 'linkedin', 'public_url']:
            if col in df.columns:
                urls = df[col].dropna().tolist()
                break
        if not urls and len(df.columns) > 0:
            urls = df.iloc[:, 0].dropna().tolist()

    # Filter to only LinkedIn URLs and normalize them
    normalized = []
    for u in urls:
        if u and 'linkedin.com' in str(u):
            u = str(u).strip()
            if u.startswith('www.'):
                u = 'https://' + u
            elif not u.startswith('http'):
                u = 'https://' + u
            normalized.append(u)
    return normalized


def enrich_batch(urls: list[str], api_key: str) -> list[dict]:
    """Enrich a batch of URLs via Crust Data API."""
    batch_str = ','.join(urls)

    try:
        response = requests.get(
            'https://api.crustdata.com/screener/person/enrich',
            params={'linkedin_profile_url': batch_str},
            headers={'Authorization': f'Token {api_key}'},
            timeout=120
        )

        if response.status_code == 200:
            data = response.json()
            if isinstance(data, list):
                return data
            return [data]
        else:
            return [{'error': response.text, 'linkedin_url': u} for u in urls]

    except Exception as e:
        return [{'error': str(e), 'linkedin_url': u} for u in urls]


def flatten_for_csv(data: list[dict]) -> pd.DataFrame:
    """Flatten nested data for CSV export."""
    flat_records = []

    for record in data:
        flat = {}
        for key, value in record.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, (list, dict)):
                        flat[f"{key}_{sub_key}"] = json.dumps(sub_value)
                    else:
                        flat[f"{key}_{sub_key}"] = sub_value
            elif isinstance(value, list):
                flat[key] = json.dumps(value)
            else:
                flat[key] = value
        flat_records.append(flat)

    return pd.DataFrame(flat_records)


# ========== PRE-FILTERING FUNCTIONS ==========

def filter_csv_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Filter CSV to keep only screening-relevant columns."""

    def calc_years_to_today(date_str):
        if pd.isna(date_str) or not str(date_str).strip():
            return None
        try:
            dt = datetime.strptime(str(date_str).strip(), '%d %b %Y')
            years = (datetime.now() - dt).days / 365.25
            return round(years, 1)
        except:
            return None

    def calc_years_between(start_str, end_str):
        if pd.isna(start_str) or not str(start_str).strip():
            return None
        try:
            start = datetime.strptime(str(start_str).strip(), '%d %b %Y')
            if pd.isna(end_str) or not str(end_str).strip():
                end = datetime.now()
            else:
                end = datetime.strptime(str(end_str).strip(), '%d %b %Y')
            years = (end - start).days / 365.25
            return round(years, 1)
        except:
            return None

    result = pd.DataFrame()

    # Basic info
    result['first_name'] = df.get('first_name', pd.Series([''] * len(df)))
    result['last_name'] = df.get('last_name', pd.Series([''] * len(df)))
    result['headline'] = df.get('headline', pd.Series([''] * len(df)))
    result['location'] = df.get('location', pd.Series([''] * len(df)))
    result['summary'] = df.get('summary', pd.Series([''] * len(df)))

    # Current position
    result['current_title'] = df.get('job_1_job_title', pd.Series([''] * len(df)))
    result['current_company'] = df.get('job_1_job_company_name', pd.Series([''] * len(df)))
    result['current_start_date'] = df.get('job_1_job_start_date', pd.Series([''] * len(df)))
    result['current_years_in_role'] = df.get('job_1_job_start_date', pd.Series()).apply(calc_years_to_today)
    result['current_description'] = df.get('job_1_job_description', pd.Series([''] * len(df)))

    # Past positions
    def combine_past_positions(row):
        positions = []
        for i in range(2, 20):
            title_col = f'job_{i}_job_title'
            company_col = f'job_{i}_job_company_name'
            start_col = f'job_{i}_job_start_date'
            end_col = f'job_{i}_job_end_date'
            desc_col = f'job_{i}_job_description'

            if title_col in df.columns and pd.notna(row.get(title_col)):
                title = row.get(title_col, '')
                company = row.get(company_col, '')
                start = row.get(start_col, '')
                end = row.get(end_col, '')
                desc = row.get(desc_col, '')
                years = calc_years_between(start, end)
                years_str = f" [{years} yrs]" if years else ""
                desc_str = f": {desc}" if pd.notna(desc) and desc else ""
                positions.append(f"{title} at {company} ({start} - {end}){years_str}{desc_str}")
        return ' || '.join(positions)

    result['past_positions'] = df.apply(combine_past_positions, axis=1)

    # Education
    def combine_education(row):
        educations = []
        for i in range(1, 10):
            school_col = f'edu_{i}_school_name'
            degree_col = f'edu_{i}_degree'
            field_col = f'edu_{i}_field_of_study'

            if school_col in df.columns and pd.notna(row.get(school_col)):
                school = row.get(school_col, '')
                degree = row.get(degree_col, '')
                field = row.get(field_col, '')
                parts = [school]
                if pd.notna(degree) and degree:
                    parts.append(degree)
                if pd.notna(field) and field:
                    parts.append(f"in {field}")
                educations.append(', '.join(parts))
        return ' | '.join(educations)

    result['education'] = df.apply(combine_education, axis=1)

    # Skills
    skill_cols = [col for col in df.columns if re.match(r'^skill_\d+_name$', col)]
    def combine_skills(row):
        skills = []
        for col in skill_cols:
            if pd.notna(row.get(col)):
                skills.append(str(row[col]))
        return ', '.join(skills)

    result['skills'] = df.apply(combine_skills, axis=1)

    # LinkedIn URL
    result['public_url'] = df.get('public_url', df.get('linkedin_url', pd.Series([''] * len(df))))

    return result


def apply_pre_filters(df: pd.DataFrame, filters: dict) -> tuple[pd.DataFrame, dict, dict]:
    """Apply pre-filters to candidates. Returns filtered df, stats, and filtered_out dict."""
    stats = {}
    filtered_out = {}  # Store removed candidates by filter type
    original_count = len(df)

    df = df.copy()  # Avoid SettingWithCopyWarning

    # Helper functions
    def normalize_company(name):
        """Normalize company name for comparison."""
        if pd.isna(name) or not str(name).strip():
            return ''
        # Lowercase and strip
        name = str(name).lower().strip()
        # Remove common suffixes
        for suffix in [' ltd', ' inc', ' corp', ' llc', ' limited', ' israel', ' il', ' technologies', ' tech', ' software', ' solutions', ' group']:
            if name.endswith(suffix):
                name = name[:-len(suffix)].strip()
        return name

    def matches_list(company, company_list):
        """Check if company matches any in the list - uses normalized exact match."""
        if pd.isna(company) or not str(company).strip():
            return False
        company_norm = normalize_company(company)
        if not company_norm:
            return False
        for c in company_list:
            c_norm = normalize_company(c)
            if not c_norm:
                continue
            # Exact match after normalization
            if company_norm == c_norm:
                return True
            # One contains the other fully (for cases like "Bank Leumi" vs "Bank Leumi Le-Israel")
            if len(c_norm) >= 4 and len(company_norm) >= 4:
                if company_norm.startswith(c_norm) or c_norm.startswith(company_norm):
                    return True
        return False

    def matches_list_in_text(text, company_list):
        """Check if any company from list appears in text - uses word boundary matching."""
        if pd.isna(text) or not str(text).strip():
            return False
        text_lower = str(text).lower()
        for c in company_list:
            c_norm = normalize_company(c)
            if not c_norm or len(c_norm) < 3:
                continue
            # Use word boundary pattern for longer names
            pattern = r'\b' + re.escape(c_norm) + r'\b'
            if re.search(pattern, text_lower):
                return True
        return False

    # ========== EXCLUSION FILTERS ==========

    # 1. Past candidates filter
    if filters.get('past_candidates_df') is not None:
        past_df = filters['past_candidates_df']
        if 'Name' in past_df.columns:
            past_names = set(str(name).lower().strip() for name in past_df['Name'].dropna())
            df['_full_name'] = (df['first_name'].fillna('').str.lower().str.strip() + ' ' +
                               df['last_name'].fillna('').str.lower().str.strip())
            df['_is_past'] = df['_full_name'].isin(past_names)
            stats['past_candidates'] = df['_is_past'].sum()
            filtered_out['Past Candidates'] = df[df['_is_past']].drop(columns=['_is_past', '_full_name']).copy()
            df = df[~df['_is_past']].drop(columns=['_is_past', '_full_name'])

    # 2. Blacklist filter
    if filters.get('blacklist'):
        blacklist = [c.lower().strip() for c in filters['blacklist']]
        df['_blacklisted'] = df['current_company'].apply(lambda x: matches_list(x, blacklist))
        stats['blacklist'] = df['_blacklisted'].sum()
        filtered_out['Blacklist Companies'] = df[df['_blacklisted']].drop(columns=['_blacklisted']).copy()
        df = df[~df['_blacklisted']].drop(columns=['_blacklisted'])

    # 3. Not relevant companies (current)
    if filters.get('not_relevant'):
        not_relevant = [c.lower().strip() for c in filters['not_relevant']]
        df['_not_relevant'] = df['current_company'].apply(lambda x: matches_list(x, not_relevant))
        stats['not_relevant_current'] = df['_not_relevant'].sum()
        filtered_out['Not Relevant (Current)'] = df[df['_not_relevant']].drop(columns=['_not_relevant']).copy()
        df = df[~df['_not_relevant']].drop(columns=['_not_relevant'])

    # 4. Not relevant companies (past)
    if filters.get('not_relevant_past') and filters.get('not_relevant'):
        not_relevant = [c.lower().strip() for c in filters['not_relevant']]
        df['_past_not_relevant'] = df['past_positions'].apply(lambda x: matches_list_in_text(x, not_relevant))
        stats['not_relevant_past'] = df['_past_not_relevant'].sum()
        filtered_out['Not Relevant (Past)'] = df[df['_past_not_relevant']].drop(columns=['_past_not_relevant']).copy()
        df = df[~df['_past_not_relevant']].drop(columns=['_past_not_relevant'])

    # 5. Job hoppers filter - exempt priority candidates
    if filters.get('filter_job_hoppers'):
        def count_short_stints(past_positions):
            if pd.isna(past_positions) or not str(past_positions).strip():
                return 0
            years_pattern = r'\[(\d+\.?\d*)\s*yrs?\]'
            matches = re.findall(years_pattern, str(past_positions))
            return sum(1 for y in matches if float(y) < 1.0)

        df['_short_stints'] = df['past_positions'].apply(count_short_stints)
        df['_is_job_hopper'] = df['_short_stints'] >= 2
        stats['job_hoppers'] = df['_is_job_hopper'].sum()
        filtered_out['Job Hoppers'] = df[df['_is_job_hopper']].drop(columns=['_short_stints', '_is_job_hopper']).copy()
        df = df[~df['_is_job_hopper']].drop(columns=['_short_stints', '_is_job_hopper'])

    # 6. Consulting companies filter
    if filters.get('filter_consulting'):
        consulting = ['tikal', 'matrix', 'ness', 'sela', 'malam', 'bynet', 'sqlink', 'john bryce',
                      'experis', 'manpower', 'infosys', 'tata', 'wipro', 'cognizant', 'accenture', 'capgemini']
        df['_is_consulting'] = df['current_company'].apply(lambda x: matches_list(x, consulting))
        stats['consulting'] = df['_is_consulting'].sum()
        filtered_out['Consulting Companies'] = df[df['_is_consulting']].drop(columns=['_is_consulting']).copy()
        df = df[~df['_is_consulting']].drop(columns=['_is_consulting'])

    # 7. Long tenure filter
    if filters.get('filter_long_tenure'):
        df['_long_tenure'] = df['current_years_in_role'].apply(lambda x: x >= 8 if pd.notna(x) else False)
        stats['long_tenure'] = df['_long_tenure'].sum()
        filtered_out['Long Tenure (8+ years)'] = df[df['_long_tenure']].drop(columns=['_long_tenure']).copy()
        df = df[~df['_long_tenure']].drop(columns=['_long_tenure'])

    # 8. Management titles filter
    if filters.get('filter_management'):
        exclude_titles = ['director', 'head of', 'vp ', 'vice president', 'cto', 'ceo', 'coo',
                          'chief ', 'group manager', 'engineering manager', 'r&d manager', 'founder']
        keep_titles = ['team lead', 'tech lead', 'staff', 'principal', 'senior', 'architect']

        def is_management_title(title):
            if pd.isna(title) or not str(title).strip():
                return False
            title_lower = str(title).lower()
            for keep in keep_titles:
                if keep in title_lower:
                    return False
            for excl in exclude_titles:
                if excl in title_lower:
                    return True
            return False

        df['_is_management'] = df['current_title'].apply(is_management_title)
        stats['management_titles'] = df['_is_management'].sum()
        filtered_out['Management Titles'] = df[df['_is_management']].drop(columns=['_is_management']).copy()
        df = df[~df['_is_management']].drop(columns=['_is_management'])

    stats['original'] = original_count
    stats['final'] = len(df)
    stats['total_removed'] = original_count - len(df)

    return df, stats, filtered_out


# Screening system prompt based on the /screen skill
SCREENING_SYSTEM_PROMPT = """You are an expert senior technical recruiter with 15+ years of experience hiring for top tech companies.

## Scoring Rubric
- **9-10**: Exceptional match. Meets all requirements with bonus qualifications. Rare.
- **7-8**: Strong match. Meets all core requirements. Top 20% of candidates.
- **5-6**: Partial match. Missing 1-2 requirements but has potential.
- **3-4**: Weak match. Missing multiple requirements.
- **1-2**: Not a fit. Don't waste time.

## Score Boosters (+1 to +2 points)
Give higher scores when candidate has:

1. **Strong Company Background**: Currently or recently at well-known tech companies:
   - Top Israeli startups: Wiz, Snyk, Monday, Gong, AppsFlyer, Fireblocks, Rapyd, etc.
   - Award winners: RSA Innovation Sandbox, Y Combinator, top-tier VC backed
   - Big tech: Google, Meta, Amazon, Microsoft (in engineering roles)
   - Acquired startups at good valuations

2. **Relevant Education**: CS/Software Engineering degree from strong universities:
   - Israel: Technion, Tel Aviv University, Hebrew University, Ben-Gurion, Bar-Ilan, Weizmann
   - Global: MIT, Stanford, CMU, Berkeley, etc.
   - MSc/PhD in CS is a plus

**Important**: If candidate is from a strong company with relevant education, skills/description matter less - the company already vetted them.

## Auto-Disqualifiers (Score 3 or below)
- **Only .NET/C#**: No Node, Python, or modern backend stack
- **Group Manager/Director+**: Not hands-on (Team Lead is OK)
- **Embedded/Systems engineer**: Wrong domain (C++, firmware, kernel)
- **QA/Automation background**: Limited backend development depth
- **Consulting/project companies**: Tikal, Matrix, Ness, Sela, etc.

## Guidelines
1. **Be Direct**: Don't sugarcoat. Give honest evaluations.
2. **Use Evidence**: Reference specific profile data (years, skills, companies).
3. **Be Calibrated**: A 10/10 should be rare. Most good candidates are 6-8.
4. **Company > Skills**: Strong company pedigree compensates for skill list gaps."""


def screen_profile(profile: dict, job_description: str, client: OpenAI, extra_requirements: str = "") -> dict:
    """Screen a profile against a job description using OpenAI."""

    # Build concise profile summary for the prompt (with safe string conversion)
    def safe_str(value, max_len=500):
        if value is None:
            return 'N/A'
        if isinstance(value, (list, dict)):
            value = json.dumps(value, ensure_ascii=False)
        return str(value)[:max_len] if value else 'N/A'

    profile_summary = f"""Name: {safe_str(profile.get('first_name', ''))} {safe_str(profile.get('last_name', ''))}
Title: {safe_str(profile.get('current_title', profile.get('headline', 'N/A')))}
Company: {safe_str(profile.get('current_company', 'N/A'))}
Location: {safe_str(profile.get('location', 'N/A'))}
Education: {safe_str(profile.get('education', 'N/A'))}
Skills: {safe_str(profile.get('skills', 'N/A'))}
Summary: {safe_str(profile.get('summary', 'N/A'))}
Past Positions: {safe_str(profile.get('past_positions', 'N/A'))}"""

    user_prompt = f"""Evaluate this candidate against the job description.

## Job Description:
{job_description}

{f"## Extra Requirements:{chr(10)}{extra_requirements}" if extra_requirements else ""}

## Candidate Profile:
{profile_summary}

Respond with ONLY valid JSON in this exact format:
{{"score": <1-10>, "fit": "<Strong Fit|Good Fit|Partial Fit|Not a Fit>", "summary": "<2-3 sentences about the candidate>", "why": "<2-3 sentences explaining the score>", "strengths": ["<strength1>", "<strength2>"], "concerns": ["<concern1>", "<concern2>"]}}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SCREENING_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )

        content = response.choices[0].message.content.strip()
        # Handle potential markdown code blocks
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()

        result = json.loads(content)
        return result
    except json.JSONDecodeError as e:
        return {
            "score": 0,
            "fit": "Error",
            "summary": f"JSON parse error",
            "why": str(e)[:100],
            "strengths": [],
            "concerns": []
        }
    except Exception as e:
        return {
            "score": 0,
            "fit": "Error",
            "summary": f"API error: {str(e)[:50]}",
            "why": str(e)[:100],
            "strengths": [],
            "concerns": []
        }


def screen_profiles_batch(profiles: list, job_description: str, openai_api_key: str,
                          extra_requirements: str = "", max_workers: int = 50,
                          progress_placeholder=None) -> list:
    """Screen multiple profiles in parallel using ThreadPoolExecutor.

    Args:
        profiles: List of profile dicts to screen
        job_description: The job description to screen against
        openai_api_key: OpenAI API key (we create client per thread for safety)
        extra_requirements: Additional screening criteria
        max_workers: Number of concurrent threads (default 50 for Tier 3)
        progress_placeholder: Streamlit placeholder for progress updates

    Returns:
        List of screening results with profile info included
    """
    results = []
    completed_count = [0]  # Use list to allow modification in nested function
    lock = threading.Lock()
    total = len(profiles)

    def screen_single(profile, index):
        # Create client per thread to avoid thread-safety issues
        client = OpenAI(api_key=openai_api_key)
        try:
            result = screen_profile(profile, job_description, client, extra_requirements)
            # Add profile info to result
            name = f"{profile.get('first_name', '')} {profile.get('last_name', '')}".strip()
            if not name:
                name = profile.get('full_name', '') or profile.get('fullName', '') or f"Profile {index}"
            result['name'] = name
            result['current_title'] = profile.get('current_title', '') or profile.get('headline', '') or profile.get('title', '') or ''
            result['current_company'] = profile.get('current_company', '') or profile.get('companyName', '') or profile.get('company', '') or ''
            result['linkedin_url'] = profile.get('public_url', '') or profile.get('linkedInProfileUrl', '') or profile.get('profileUrl', '') or ''
            result['index'] = index
        except Exception as e:
            import traceback
            result = {
                "score": 0,
                "fit": "Error",
                "summary": f"Screen error: {str(e)[:80]}",
                "why": traceback.format_exc()[:200],
                "strengths": [],
                "concerns": [],
                "name": profile.get('first_name', '') or profile.get('fullName', '') or f"Profile {index}",
                "current_title": "",
                "current_company": "",
                "linkedin_url": "",
                "index": index
            }

        with lock:
            completed_count[0] += 1

        return result

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_index = {
            executor.submit(screen_single, profile, i): i
            for i, profile in enumerate(profiles)
        }

        # Collect results as they complete
        for future in as_completed(future_to_index):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                import traceback
                idx = future_to_index[future]
                error_msg = str(e) if str(e) else "Unknown error"
                results.append({
                    "score": 0,
                    "fit": "Error",
                    "summary": f"Thread error: {error_msg[:80]}",
                    "why": traceback.format_exc()[:200],
                    "strengths": [],
                    "concerns": [],
                    "name": f"Profile {idx}",
                    "current_title": "",
                    "current_company": "",
                    "linkedin_url": "",
                    "index": idx
                })

    # Sort by original index to maintain order
    results.sort(key=lambda x: x.get('index', 0))
    return results


# Main UI
st.title("LinkedIn Profile Enricher")

# Check API keys
api_key = load_api_key()
has_crust_key = api_key and api_key != "YOUR_CRUSTDATA_API_KEY_HERE"

# Show data status in header
if 'results' in st.session_state and st.session_state['results']:
    st.info(f"ðŸ“Š **{len(st.session_state['results'])}** profiles loaded")

# Create tabs
tab_upload, tab_filter, tab_results, tab_screening = st.tabs([
    "ðŸ“¤ Upload", "ðŸ” Filter", "ðŸ“‹ Results", "ðŸ¤– AI Screening"
])

# ========== TAB 1: Upload ==========
with tab_upload:
    pb_key = load_phantombuster_key()
    has_pb_key = pb_key and pb_key != "YOUR_PHANTOMBUSTER_API_KEY_HERE"

    if has_pb_key:
        # Fetch all agents once
        agents = fetch_phantombuster_agents(pb_key)

        # ===== SECTION 1: Load from PhantomBuster =====
        st.markdown("### Load from PhantomBuster")

        if agents:
            agent_names = [a['name'] for a in agents]

            selected_agent_name = st.selectbox(
                "Select Phantom",
                options=agent_names,
                key="pb_agent_select"
            )

            selected_agent = next((a for a in agents if a['name'] == selected_agent_name), None)

            if selected_agent:
                # File selection options
                col_load_mode, col_refresh = st.columns([3, 1])

                with col_load_mode:
                    load_mode = st.radio(
                        "Load mode",
                        options=["Latest results", "Select specific file"],
                        horizontal=True,
                        key="pb_load_mode",
                        help="Choose to load all accumulated results or a specific search file"
                    )

                # List files if specific file mode selected
                available_files = []
                selected_file = None

                if load_mode == "Select specific file":
                    with col_refresh:
                        if st.button("ðŸ”„", key="pb_refresh_files", help="Refresh file list"):
                            st.session_state['pb_files_cache'] = None

                    # Cache files list to avoid repeated API calls
                    cache_key = f"pb_files_{selected_agent['id']}"
                    if cache_key not in st.session_state or st.session_state.get('pb_files_cache') is None:
                        with st.spinner("Loading files..."):
                            available_files = list_phantombuster_files(pb_key, selected_agent['id'])
                            st.session_state[cache_key] = available_files
                            st.session_state['pb_files_cache'] = True
                    else:
                        available_files = st.session_state.get(cache_key, [])

                    if available_files:
                        # Sort by name (most recent first for timestamped files)
                        available_files.sort(key=lambda x: x['name'], reverse=True)
                        file_names = [f['name'] for f in available_files]

                        selected_file = st.selectbox(
                            "Select file",
                            options=file_names,
                            key="pb_file_select",
                            help="Select a specific result file to load"
                        )

                        # Show file info
                        file_info = next((f for f in available_files if f['name'] == selected_file), None)
                        if file_info and file_info.get('size'):
                            size_kb = file_info['size'] / 1024
                            st.caption(f"Size: {size_kb:.1f} KB")
                    else:
                        st.info("No result files found. Run a search first.")

                if st.button("Load Results", type="primary", key="pb_load_btn", use_container_width=True):
                    with st.spinner("Loading results..."):
                        # Pass specific filename if selected
                        filename = selected_file.replace('.csv', '').replace('.json', '') if selected_file else None
                        pb_df = fetch_phantombuster_result_csv(pb_key, selected_agent['id'], debug=False, filename=filename)
                        if not pb_df.empty:
                            pb_df = normalize_phantombuster_columns(pb_df)
                            st.session_state['results'] = pb_df.to_dict('records')
                            st.session_state['results_df'] = pb_df
                            file_msg = f" from **{selected_file}**" if selected_file else ""
                            st.success(f"Loaded **{len(pb_df)}** profiles{file_msg}!")
                            st.rerun()
                        else:
                            st.error("No results found.")
                            st.info("Use **Launch Search** below to run a new search.")
        else:
            st.warning("No phantoms found in your PhantomBuster account")

        st.divider()

        # ===== SECTION 2: Launch Search =====
        st.markdown("### Launch Search")
        st.caption("Launch your personal phantom with a Sales Navigator search URL")

        # Initialize session state
        if 'pb_launch_status' not in st.session_state:
            st.session_state['pb_launch_status'] = 'idle'
        if 'pb_launch_agent_id' not in st.session_state:
            st.session_state['pb_launch_agent_id'] = None
        if 'pb_launch_container_id' not in st.session_state:
            st.session_state['pb_launch_container_id'] = None
        if 'pb_launch_error' not in st.session_state:
            st.session_state['pb_launch_error'] = None
        if 'pb_launch_start_time' not in st.session_state:
            st.session_state['pb_launch_start_time'] = None

        # User name input
        user_name = st.text_input(
            "Your name",
            value=st.session_state.get('pb_user_name', ''),
            placeholder="e.g., John",
            key="pb_user_name_input"
        )

        if user_name:
            st.session_state['pb_user_name'] = user_name

        # Sales Navigator URL input
        search_url = st.text_input(
            "Sales Navigator Search URL",
            placeholder="https://www.linkedin.com/sales/search/people?...",
            key="pb_search_url"
        )

        # Find user's phantom
        user_phantom = None
        if user_name and agents:
            user_name_lower = user_name.lower()
            for agent in agents:
                if user_name_lower in agent.get('name', '').lower():
                    user_phantom = agent
                    break

            if user_phantom:
                st.success(f"Found your phantom: **{user_phantom['name']}**")
            else:
                st.warning(f"No phantom found with '{user_name}' in the name")

        # Launch status and buttons
        current_status = st.session_state['pb_launch_status']

        if current_status == 'running':
            # Auto-fetch latest status
            container_id = st.session_state['pb_launch_container_id']
            if container_id:
                status_result = fetch_container_status(pb_key, container_id)
                container_status = status_result.get('status', 'unknown')

                # Store progress info
                st.session_state['pb_progress_info'] = {
                    'profiles_count': status_result.get('profiles_count', 0),
                    'progress_pct': status_result.get('progress_pct', 0),
                    'progress': status_result.get('progress'),
                }

                # Check if finished or error
                if container_status == 'finished':
                    st.session_state['pb_launch_status'] = 'finished'
                    # Desktop notification
                    try:
                        profiles = status_result.get('profiles_count', 0)
                        msg = f"Extracted {profiles} profiles" if profiles else "Ready to load results"
                        notification.notify(
                            title="PhantomBuster Finished",
                            message=msg,
                            app_name="LinkedIn Enricher",
                            timeout=10
                        )
                        # Windows sound
                        try:
                            winsound.MessageBeep(winsound.MB_OK)
                        except:
                            pass
                    except:
                        pass
                    st.rerun()
                elif container_status == 'error':
                    st.session_state['pb_launch_status'] = 'error'
                    st.session_state['pb_launch_error'] = status_result.get('exitMessage', 'Phantom failed')
                    # Desktop notification for error
                    try:
                        notification.notify(
                            title="PhantomBuster Error",
                            message=status_result.get('exitMessage', 'Phantom failed'),
                            app_name="LinkedIn Enricher",
                            timeout=10
                        )
                        try:
                            winsound.MessageBeep(winsound.MB_ICONHAND)
                        except:
                            pass
                    except:
                        pass
                    st.rerun()

            # Show running status with progress
            elapsed = ""
            if st.session_state['pb_launch_start_time']:
                elapsed_seconds = int(time.time() - st.session_state['pb_launch_start_time'])
                elapsed_min = elapsed_seconds // 60
                elapsed_sec = elapsed_seconds % 60
                elapsed = f"{elapsed_min}m {elapsed_sec}s"

            # Display progress info
            progress_info = st.session_state.get('pb_progress_info', {})
            profiles_count = progress_info.get('profiles_count', 0)
            progress_pct = progress_info.get('progress_pct', 0)

            # Progress display
            st.info(f"**Running** - {elapsed} (auto-refreshing every 10s)")
            if profiles_count > 0 or progress_pct > 0:
                col_prog1, col_prog2 = st.columns(2)
                with col_prog1:
                    if profiles_count > 0:
                        st.metric("Profiles extracted", profiles_count)
                with col_prog2:
                    if progress_pct > 0:
                        st.metric("Progress", f"{progress_pct}%")

            # Show progress bar if we have percentage
            if progress_pct > 0:
                st.progress(progress_pct / 100)

            # Cancel button
            if st.button("Cancel", key="pb_cancel_btn"):
                st.session_state['pb_launch_status'] = 'idle'
                st.session_state['pb_launch_container_id'] = None
                st.session_state['pb_launch_start_time'] = None
                st.session_state['pb_progress_info'] = {}
                st.rerun()

            # Auto-refresh every 10 seconds
            time.sleep(10)
            st.rerun()

        elif current_status == 'finished':
            progress_info = st.session_state.get('pb_progress_info', {})
            profiles_count = progress_info.get('profiles_count', 0)
            csv_name = st.session_state.get('pb_launch_csv_name')

            if profiles_count > 0:
                st.success(f"Phantom finished! Extracted **{profiles_count}** profiles")
            else:
                st.success("Phantom finished!")

            if csv_name:
                st.info(f"Results saved to: **{csv_name}.csv**")

            if st.button("Load Results", type="primary", key="pb_load_results_btn"):
                agent_id = st.session_state['pb_launch_agent_id']
                with st.spinner("Loading results..."):
                    # Load the specific file created during this launch
                    pb_df = fetch_phantombuster_result_csv(pb_key, agent_id, filename=csv_name)
                    if not pb_df.empty:
                        pb_df = normalize_phantombuster_columns(pb_df)
                        st.session_state['results'] = pb_df.to_dict('records')
                        st.session_state['results_df'] = pb_df
                        st.session_state['pb_launch_status'] = 'idle'
                        st.session_state['pb_launch_container_id'] = None
                        st.session_state['pb_launch_start_time'] = None
                        st.session_state['pb_launch_csv_name'] = None
                        st.session_state['pb_progress_info'] = {}
                        file_msg = f" from **{csv_name}.csv**" if csv_name else ""
                        st.success(f"Loaded **{len(pb_df)}** profiles{file_msg}!")
                        st.rerun()
                    else:
                        st.error("Could not load results.")

        elif current_status == 'error':
            st.error(f"Error: {st.session_state['pb_launch_error']}")
            if st.button("Reset", key="pb_reset_btn"):
                st.session_state['pb_launch_status'] = 'idle'
                st.session_state['pb_launch_error'] = None
                st.session_state['pb_launch_csv_name'] = None
                st.session_state['pb_progress_info'] = {}
                st.rerun()

        else:  # idle
            can_launch = user_phantom and search_url
            if st.button("Launch", type="primary", key="pb_launch_btn", disabled=not can_launch):
                st.session_state['pb_launch_status'] = 'launching'
                st.session_state['pb_launch_agent_id'] = user_phantom['id']
                st.session_state['pb_launch_error'] = None

                # Update phantom with new search URL and timestamped output filename
                update_result = update_phantombuster_search_url(pb_key, user_phantom['id'], search_url, 2500)
                if update_result.get('success'):
                    # Store the generated filename for later retrieval
                    st.session_state['pb_launch_csv_name'] = update_result.get('csvName')

                    # Launch without clearing results - new file will be created with unique name
                    result = launch_phantombuster_agent(pb_key, user_phantom['id'], None, clear_results=False)

                    if 'containerId' in result:
                        st.session_state['pb_launch_container_id'] = result['containerId']
                        st.session_state['pb_launch_status'] = 'running'
                        st.session_state['pb_launch_start_time'] = time.time()
                    else:
                        st.session_state['pb_launch_status'] = 'error'
                        st.session_state['pb_launch_error'] = result.get('error', 'Unknown error')
                else:
                    st.session_state['pb_launch_status'] = 'error'
                    st.session_state['pb_launch_error'] = update_result.get('error', 'Failed to update search URL')

                st.rerun()

            if not user_name:
                st.info("Enter your name to find your phantom")
            elif not user_phantom:
                st.info("No phantom found - ask admin to create one for you")
            elif not search_url:
                st.info("Paste a Sales Navigator search URL to launch")

        # ===== Advanced Settings =====
        with st.expander("Advanced Settings"):
            st.markdown("**Upload CSV manually**")
            st.caption("Download CSV from PhantomBuster dashboard and upload here")
            pb_upload = st.file_uploader(
                "Upload PhantomBuster CSV",
                type=['csv'],
                key="pb_manual_upload"
            )

            if pb_upload:
                try:
                    pb_df = pd.read_csv(pb_upload)
                    if not pb_df.empty:
                        pb_df = normalize_phantombuster_columns(pb_df)
                        st.session_state['results'] = pb_df.to_dict('records')
                        st.session_state['results_df'] = pb_df
                        st.success(f"Loaded **{len(pb_df)}** profiles!")
                        st.rerun()
                except Exception as e:
                    st.error(f"Error reading CSV: {e}")

        st.divider()

    # ===== File Upload Section =====
    st.markdown("### Upload File")
    pre_enriched_file = st.file_uploader(
        "Upload pre-enriched CSV or JSON",
        type=['csv', 'json'],
        key="pre_enriched_upload"
    )

    if pre_enriched_file:
        try:
            if pre_enriched_file.name.endswith('.json'):
                pre_enriched_data = json.load(pre_enriched_file)
                if isinstance(pre_enriched_data, list):
                    st.session_state['results'] = pre_enriched_data
                    st.session_state['results_df'] = flatten_for_csv(pre_enriched_data)
                    st.success(f"Loaded **{len(pre_enriched_data)}** profiles!")
            else:
                pre_enriched_file.seek(0)
                df_uploaded = pd.read_csv(pre_enriched_file, encoding='utf-8')
                st.session_state['results'] = df_uploaded.to_dict('records')
                st.session_state['results_df'] = df_uploaded
                st.success(f"Loaded **{len(df_uploaded)}** profiles!")
        except Exception as e:
            st.error(f"Error: {e}")

    # ===== Data Preview =====
    if 'results' in st.session_state and st.session_state['results']:
        st.divider()
        st.markdown("### Loaded Data Preview")
        df = st.session_state['results_df']
        st.success(f"**{len(df)}** profiles loaded")
        display_cols = ['first_name', 'last_name', 'current_title', 'current_company', 'education', 'location']
        available_cols = [c for c in display_cols if c in df.columns]
        st.dataframe(df[available_cols].head(50) if available_cols else df.head(50), use_container_width=True, hide_index=True)
        st.caption(f"Showing first 50 of {len(df)} profiles")

# ========== TAB 2: Filter ==========
with tab_filter:
    if 'results' not in st.session_state or not st.session_state['results']:
        st.info("Upload data in the Upload tab first.")
    else:
        df = st.session_state['results_df']
        needs_filtering = 'job_1_job_title' in df.columns and 'current_title' not in df.columns

        if needs_filtering:
            if st.button("Convert to Screening Format"):
                filtered_df = filter_csv_columns(df)
                st.session_state['results_df'] = filtered_df
                st.session_state['results'] = filtered_df.to_dict('records')
                st.rerun()

        # Check for Google Sheets config
        filter_sheets = get_filter_sheets_config().copy()
        gspread_client = get_gspread_client()

        # Allow user to specify their own filter sheet
        st.markdown("**Your Filter Sheet:**")
        st.caption(f"Share your sheet with: `linkedin-enricher@linkedin-enricher-485616.iam.gserviceaccount.com`")
        user_sheet_url = st.text_input(
            "Google Sheet URL (paste your own, or leave empty for default)",
            value=st.session_state.get('user_sheet_url', ''),
            placeholder="https://docs.google.com/spreadsheets/d/...",
            key="user_sheet_input"
        )

        # Save to session state
        if user_sheet_url:
            st.session_state['user_sheet_url'] = user_sheet_url
            filter_sheets['url'] = user_sheet_url

        has_sheets = bool(filter_sheets.get('url')) and gspread_client is not None

        if has_sheets:
            if user_sheet_url:
                st.success("Using your personal filter sheet")
            else:
                st.success("Using default filter sheet")

            # Validate sheet connection
            if st.button("Verify Sheet Connection", key="verify_sheet"):
                with st.spinner("Checking sheet access..."):
                    try:
                        spreadsheet = gspread_client.open_by_url(filter_sheets['url'])
                        tabs = [ws.title for ws in spreadsheet.worksheets()]
                        st.success(f"Connected! Found {len(tabs)} tabs")
                        expected_tabs = ['Past Candidates', 'Blacklist', 'NotRelevant Companies', 'Target Companies', 'Universities', 'Tech Alerts']
                        missing = [t for t in expected_tabs if t not in tabs]
                        if missing:
                            st.warning(f"Missing expected tabs: {', '.join(missing)}")
                        else:
                            st.info(f"All expected tabs found")
                    except Exception as e:
                        st.error(f"Cannot access sheet: {e}")
                        st.info("Make sure you shared the sheet with the service account email above")

        st.divider()
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Filter Data Sources:**")

            # Past candidates
            use_sheets_past = False
            past_candidates_file = None
            if has_sheets and filter_sheets.get('past_candidates'):
                use_sheets_past = st.checkbox("Load Past Candidates from Google Sheet", value=True, key="use_sheets_past")
                if not use_sheets_past:
                    past_candidates_file = st.file_uploader("Past Candidates CSV", type=['csv'], key="past_candidates")
            else:
                past_candidates_file = st.file_uploader("Past Candidates CSV", type=['csv'], key="past_candidates")

            # Blacklist
            use_sheets_blacklist = False
            blacklist_file = None
            if has_sheets and filter_sheets.get('blacklist'):
                use_sheets_blacklist = st.checkbox("Load Blacklist from Google Sheet", value=True, key="use_sheets_blacklist")
                if not use_sheets_blacklist:
                    blacklist_file = st.file_uploader("Blacklist Companies CSV", type=['csv'], key="blacklist")
            else:
                blacklist_file = st.file_uploader("Blacklist Companies CSV", type=['csv'], key="blacklist")

            # Not relevant
            use_sheets_not_relevant = False
            not_relevant_file = None
            if has_sheets and filter_sheets.get('not_relevant'):
                use_sheets_not_relevant = st.checkbox("Load Not Relevant from Google Sheet", value=True, key="use_sheets_not_relevant")
                if not use_sheets_not_relevant:
                    not_relevant_file = st.file_uploader("Not Relevant Companies CSV", type=['csv'], key="not_relevant")
            else:
                not_relevant_file = st.file_uploader("Not Relevant Companies CSV", type=['csv'], key="not_relevant")

        with col2:
            st.markdown("**Exclusion Filters:**")
            filter_not_relevant_past = st.checkbox("Also filter past positions for not relevant", value=True)
            filter_job_hoppers = st.checkbox("Filter job hoppers (2+ roles < 1 year)", value=True)
            filter_consulting = st.checkbox("Filter consulting/project companies", value=True)
            filter_long_tenure = st.checkbox("Filter 8+ years at one company", value=True)
            filter_management = st.checkbox("Filter Director/VP/Head titles", value=True)

        if st.button("Apply Filters", type="primary"):
            filters = {
                'filter_job_hoppers': filter_job_hoppers,
                'filter_consulting': filter_consulting,
                'filter_long_tenure': filter_long_tenure,
                'filter_management': filter_management,
                'not_relevant_past': filter_not_relevant_past,
            }

            # Load filter data from Google Sheets or files
            with st.spinner("Loading filter data..."):
                sheet_url = filter_sheets.get('url', '')

                # Past candidates
                if use_sheets_past and filter_sheets.get('past_candidates'):
                    past_df = load_sheet_as_df(sheet_url, filter_sheets['past_candidates'])
                    if past_df is not None:
                        filters['past_candidates_df'] = past_df
                        st.info(f"Loaded {len(past_df)} past candidates from Google Sheet")
                elif past_candidates_file:
                    filters['past_candidates_df'] = pd.read_csv(past_candidates_file)

                # Blacklist
                if use_sheets_blacklist and filter_sheets.get('blacklist'):
                    bl_df = load_sheet_as_df(sheet_url, filter_sheets['blacklist'])
                    if bl_df is not None and len(bl_df.columns) > 0:
                        filters['blacklist'] = bl_df.iloc[:, 0].dropna().tolist()
                        st.info(f"Loaded {len(filters['blacklist'])} blacklist companies from Google Sheet")
                elif blacklist_file:
                    bl_df = pd.read_csv(blacklist_file)
                    filters['blacklist'] = bl_df.iloc[:, 0].dropna().tolist()

                # Not relevant
                if use_sheets_not_relevant and filter_sheets.get('not_relevant'):
                    nr_df = load_sheet_as_df(sheet_url, filter_sheets['not_relevant'])
                    if nr_df is not None and len(nr_df.columns) > 0:
                        filters['not_relevant'] = nr_df.iloc[:, 0].dropna().tolist()
                        st.info(f"Loaded {len(filters['not_relevant'])} not-relevant companies from Google Sheet")
                elif not_relevant_file:
                    nr_df = pd.read_csv(not_relevant_file)
                    filters['not_relevant'] = nr_df.iloc[:, 0].dropna().tolist()

            # Apply filters
            with st.spinner("Applying filters..."):
                df = st.session_state['results_df']
                filtered_df, stats, filtered_out = apply_pre_filters(df, filters)

                st.session_state['passed_candidates_df'] = filtered_df  # Store filtered results separately
                st.session_state['results_df'] = filtered_df
                st.session_state['results'] = filtered_df.to_dict('records')
                st.session_state['filter_stats'] = stats
                st.session_state['filtered_out'] = filtered_out

            st.success(f"Filtering complete! {stats['final']} candidates remaining")
            st.rerun()

    # Show filter stats if available
    if 'filter_stats' in st.session_state:
        stats = st.session_state['filter_stats']
        st.markdown("**Filter Results:**")
        stats_cols = st.columns(4)
        with stats_cols[0]:
            st.metric("Original", stats.get('original', 0))
        with stats_cols[1]:
            st.metric("Removed", stats.get('total_removed', 0))
        with stats_cols[2]:
            st.metric("Remaining", stats.get('final', 0))
        with stats_cols[3]:
            pct = round((stats.get('final', 0) / stats.get('original', 1)) * 100) if stats.get('original', 0) > 0 else 0
            st.metric("Keep Rate", f"{pct}%")

        with st.expander("Detailed Breakdown"):
            st.markdown("**Filtered Out:**")
            exclude_keys = ['original', 'final', 'total_removed']
            for key, value in stats.items():
                if key not in exclude_keys and value > 0:
                    st.text(f"  âœ— {key.replace('_', ' ').title()}: {value} removed")

    # View passed candidates section (only show after filtering)
    if 'filter_stats' in st.session_state and 'passed_candidates_df' in st.session_state:
        st.divider()
        st.markdown("### View Passed Candidates")
        st.caption("Browse candidates that passed all filters, with priority categorization")

        passed_df = st.session_state['passed_candidates_df']

        # Priority categorization section
        filter_sheets = get_filter_sheets_config()
        gspread_client = get_gspread_client()
        has_sheets = bool(filter_sheets.get('url')) and gspread_client is not None

        if has_sheets and st.button("Load Priority Categories", key="apply_categories"):
            sheet_url = filter_sheets.get('url', '')

            # Helper function for matching
            def normalize_company(name):
                if pd.isna(name) or not str(name).strip():
                    return ''
                name = str(name).lower().strip()
                for suffix in [' ltd', ' inc', ' corp', ' llc', ' limited', ' israel', ' il', ' technologies', ' tech', ' software', ' solutions', ' group']:
                    if name.endswith(suffix):
                        name = name[:-len(suffix)].strip()
                return name

            def matches_list(company, company_list):
                if pd.isna(company) or not str(company).strip():
                    return False
                company_norm = normalize_company(company)
                if not company_norm:
                    return False
                for c in company_list:
                    c_norm = normalize_company(c)
                    if not c_norm:
                        continue
                    if company_norm == c_norm:
                        return True
                    if len(c_norm) >= 4 and len(company_norm) >= 4:
                        if company_norm.startswith(c_norm) or c_norm.startswith(company_norm):
                            return True
                return False

            def matches_list_in_text(text, items_list):
                if pd.isna(text) or not str(text).strip():
                    return False
                text_lower = str(text).lower()
                for item in items_list:
                    item_norm = str(item).lower().strip()
                    if not item_norm or len(item_norm) < 3:
                        continue
                    if item_norm in text_lower:
                        return True
                return False

            with st.spinner("Loading priority lists..."):
                # Target companies
                if filter_sheets.get('target_companies'):
                    tc_df = load_sheet_as_df(sheet_url, filter_sheets['target_companies'])
                    if tc_df is not None and len(tc_df.columns) > 0:
                        target_companies = []
                        for col in tc_df.columns:
                            if 'company' in col.lower() or 'name' in col.lower():
                                target_companies.extend(tc_df[col].dropna().tolist())
                        target_list = [str(c).lower().strip() for c in target_companies if c]
                        passed_df['is_target_company'] = passed_df['current_company'].apply(lambda x: matches_list(x, target_list))
                        st.info(f"Target Companies: {len(target_list)} loaded, {passed_df['is_target_company'].sum()} matches")

                # Layoff alerts
                if filter_sheets.get('tech_alerts'):
                    ta_df = load_sheet_as_df(sheet_url, filter_sheets['tech_alerts'])
                    if ta_df is not None and len(ta_df.columns) > 0:
                        tech_alerts = []
                        for col in ta_df.columns:
                            if 'company' in col.lower() or 'name' in col.lower():
                                tech_alerts.extend(ta_df[col].dropna().tolist())
                        alerts_list = [str(c).lower().strip() for c in tech_alerts if c]
                        passed_df['is_layoff_company'] = passed_df['current_company'].apply(lambda x: matches_list(x, alerts_list))
                        st.info(f"Layoff Alerts: {len(alerts_list)} loaded, {passed_df['is_layoff_company'].sum()} matches")

                # Universities
                if filter_sheets.get('universities'):
                    uni_df = load_sheet_as_df(sheet_url, filter_sheets['universities'])
                    if uni_df is not None and len(uni_df.columns) > 0:
                        uni_list = uni_df.iloc[:, 0].dropna().tolist()
                        passed_df['is_top_university'] = passed_df['education'].apply(lambda x: matches_list_in_text(x, uni_list))
                        st.info(f"Top Universities: {len(uni_list)} loaded, {passed_df['is_top_university'].sum()} matches")

            # Save categorized data
            st.session_state['passed_candidates_df'] = passed_df
            st.session_state['categories_applied'] = True
            st.rerun()

        # Re-read from session state to get latest data with categories
        passed_df = st.session_state['passed_candidates_df']

        # Filter checkboxes at the top
        st.markdown("**Filter by category:**")
        filter_cols = st.columns(4)

        with filter_cols[0]:
            show_all = st.checkbox("All", value=True, key="filter_all")

        has_target = 'is_target_company' in passed_df.columns
        has_layoff = 'is_layoff_company' in passed_df.columns
        has_uni = 'is_top_university' in passed_df.columns

        with filter_cols[1]:
            if has_target:
                count = int(passed_df['is_target_company'].fillna(False).sum())
                show_target = st.checkbox(f"Target Companies ({count})", value=False, key="filter_target")
            else:
                show_target = False

        with filter_cols[2]:
            if has_layoff:
                count = int(passed_df['is_layoff_company'].fillna(False).sum())
                show_layoff = st.checkbox(f"Layoff Alerts ({count})", value=False, key="filter_layoff")
            else:
                show_layoff = False

        with filter_cols[3]:
            if has_uni:
                count = int(passed_df['is_top_university'].fillna(False).sum())
                show_uni = st.checkbox(f"Top Universities ({count})", value=False, key="filter_uni")
            else:
                show_uni = False

        # Filter based on checkboxes
        if show_all or (not show_target and not show_layoff and not show_uni):
            view_df = passed_df.copy()
        else:
            # Combine selected filters with OR
            mask = pd.Series([False] * len(passed_df), index=passed_df.index)
            if show_target and has_target:
                mask = mask | passed_df['is_target_company'].fillna(False)
            if show_layoff and has_layoff:
                mask = mask | passed_df['is_layoff_company'].fillna(False)
            if show_uni and has_uni:
                mask = mask | passed_df['is_top_university'].fillna(False)
            view_df = passed_df[mask].copy()

        st.success(f"**{len(view_df)}** candidates")

        # Display columns (without category columns)
        display_cols = ['first_name', 'last_name', 'current_title', 'current_company', 'education', 'location', 'public_url']
        available_cols = [c for c in display_cols if c in view_df.columns]

        if available_cols:
            st.dataframe(
                view_df[available_cols],
                use_container_width=True,
                hide_index=True,
                column_config={
                    "public_url": st.column_config.LinkColumn("LinkedIn"),
                }
            )

        # Download passed candidates
        col1, col2 = st.columns(2)
        with col1:
            csv_data = view_df.to_csv(index=False)
            st.download_button(
                label="Download View (CSV)",
                data=csv_data,
                file_name="passed_candidates.csv",
                mime="text/csv"
            )

    # Review filtered candidates section
    if 'filtered_out' in st.session_state and st.session_state['filtered_out']:
        st.divider()
        st.markdown("### Review Filtered Candidates")
        st.caption("View candidates removed by each filter and restore selected ones")

        filtered_out = st.session_state['filtered_out']
        filter_names = [k for k, v in filtered_out.items() if len(v) > 0]

        if filter_names:
            selected_filter = st.selectbox("Select filter to review:", filter_names)

            if selected_filter and selected_filter in filtered_out:
                filter_df = filtered_out[selected_filter]
                st.info(f"**{len(filter_df)}** candidates removed by: {selected_filter}")

                # Show key columns for review
                display_cols = ['first_name', 'last_name', 'current_title', 'current_company', 'education', 'public_url']
                available_cols = [c for c in display_cols if c in filter_df.columns]

                if available_cols:
                    st.dataframe(
                        filter_df[available_cols],
                        use_container_width=True,
                        hide_index=True,
                        column_config={
                            "public_url": st.column_config.LinkColumn("LinkedIn")
                        }
                    )

                # Restore functionality
                st.markdown("**Restore candidates:**")
                restore_indices = st.multiselect(
                    f"Select rows to restore (by index):",
                    options=list(range(len(filter_df))),
                    format_func=lambda i: f"{filter_df.iloc[i].get('first_name', '')} {filter_df.iloc[i].get('last_name', '')} - {filter_df.iloc[i].get('current_company', '')}"
                )

                if st.button("Restore Selected", key="restore_btn"):
                    if restore_indices:
                        # Get candidates to restore
                        to_restore = filter_df.iloc[restore_indices]

                        # Add back to main results
                        current_df = st.session_state['results_df']
                        restored_df = pd.concat([current_df, to_restore], ignore_index=True)
                        st.session_state['results_df'] = restored_df
                        st.session_state['results'] = restored_df.to_dict('records')

                        # Remove from filtered_out
                        remaining = filter_df.drop(filter_df.index[restore_indices])
                        st.session_state['filtered_out'][selected_filter] = remaining

                        st.success(f"Restored {len(restore_indices)} candidates!")
                        st.rerun()
                    else:
                        st.warning("Select candidates to restore first")
        else:
            st.info("No candidates were filtered out")

# ========== TAB 3: Results ==========
with tab_results:
    if 'filter_stats' not in st.session_state:
        st.info("Apply filters in the Filter tab first to see results.")
    elif 'passed_candidates_df' in st.session_state:
        passed_df = st.session_state['passed_candidates_df']
        stats = st.session_state['filter_stats']

        # Summary metrics
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Original", stats.get('original', 0))
        col2.metric("Removed", stats.get('total_removed', 0))
        col3.metric("Passed", stats.get('final', 0))
        col4.metric("Keep Rate", f"{round((stats.get('final', 0) / max(stats.get('original', 1), 1)) * 100)}%")

        st.divider()

        # Priority categories
        filter_sheets = get_filter_sheets_config()
        gspread_client = get_gspread_client()
        has_sheets = bool(filter_sheets.get('url')) and gspread_client is not None

        if has_sheets and st.button("Load Priority Categories", key="load_categories_results"):
            sheet_url = filter_sheets.get('url', '')

            def normalize_company(name):
                if pd.isna(name) or not str(name).strip():
                    return ''
                name = str(name).lower().strip()
                for suffix in [' ltd', ' inc', ' corp', ' llc', ' limited', ' israel', ' il', ' technologies', ' tech', ' software', ' solutions', ' group']:
                    if name.endswith(suffix):
                        name = name[:-len(suffix)].strip()
                return name

            def matches_list(company, company_list):
                if pd.isna(company) or not str(company).strip():
                    return False
                company_norm = normalize_company(company)
                if not company_norm:
                    return False
                for c in company_list:
                    c_norm = normalize_company(c)
                    if c_norm and (company_norm == c_norm or (len(c_norm) >= 4 and len(company_norm) >= 4 and (company_norm.startswith(c_norm) or c_norm.startswith(company_norm)))):
                        return True
                return False

            def matches_list_in_text(text, items_list):
                if pd.isna(text) or not str(text).strip():
                    return False
                text_lower = str(text).lower()
                return any(str(item).lower().strip() in text_lower for item in items_list if len(str(item).strip()) >= 3)

            with st.spinner("Loading priority lists..."):
                if filter_sheets.get('target_companies'):
                    tc_df = load_sheet_as_df(sheet_url, filter_sheets['target_companies'])
                    if tc_df is not None:
                        target_companies = []
                        for col in tc_df.columns:
                            if 'company' in col.lower() or 'name' in col.lower():
                                target_companies.extend(tc_df[col].dropna().tolist())
                        target_list = [str(c).lower().strip() for c in target_companies if c]
                        passed_df['is_target_company'] = passed_df['current_company'].apply(lambda x: matches_list(x, target_list))
                        st.info(f"Target Companies: {passed_df['is_target_company'].sum()} matches")

                if filter_sheets.get('tech_alerts'):
                    ta_df = load_sheet_as_df(sheet_url, filter_sheets['tech_alerts'])
                    if ta_df is not None:
                        tech_alerts = []
                        for col in ta_df.columns:
                            if 'company' in col.lower() or 'name' in col.lower():
                                tech_alerts.extend(ta_df[col].dropna().tolist())
                        alerts_list = [str(c).lower().strip() for c in tech_alerts if c]
                        passed_df['is_layoff_company'] = passed_df['current_company'].apply(lambda x: matches_list(x, alerts_list))
                        st.info(f"Layoff Alerts: {passed_df['is_layoff_company'].sum()} matches")

                if filter_sheets.get('universities'):
                    uni_df = load_sheet_as_df(sheet_url, filter_sheets['universities'])
                    if uni_df is not None:
                        uni_list = uni_df.iloc[:, 0].dropna().tolist()
                        passed_df['is_top_university'] = passed_df['education'].apply(lambda x: matches_list_in_text(x, uni_list))
                        st.info(f"Top Universities: {passed_df['is_top_university'].sum()} matches")

            st.session_state['passed_candidates_df'] = passed_df
            st.rerun()

        # Filter checkboxes
        st.markdown("**Filter by category:**")
        fcol1, fcol2, fcol3, fcol4 = st.columns(4)
        show_all = fcol1.checkbox("All", value=True, key="res_filter_all")
        show_target = fcol2.checkbox(f"Target Co ({int(passed_df['is_target_company'].sum()) if 'is_target_company' in passed_df.columns else 0})", key="res_filter_target") if 'is_target_company' in passed_df.columns else False
        show_layoff = fcol3.checkbox(f"Layoffs ({int(passed_df['is_layoff_company'].sum()) if 'is_layoff_company' in passed_df.columns else 0})", key="res_filter_layoff") if 'is_layoff_company' in passed_df.columns else False
        show_uni = fcol4.checkbox(f"Top Uni ({int(passed_df['is_top_university'].sum()) if 'is_top_university' in passed_df.columns else 0})", key="res_filter_uni") if 'is_top_university' in passed_df.columns else False

        # Apply filter
        if show_all or (not show_target and not show_layoff and not show_uni):
            view_df = passed_df
        else:
            mask = pd.Series([False] * len(passed_df), index=passed_df.index)
            if show_target and 'is_target_company' in passed_df.columns:
                mask = mask | passed_df['is_target_company'].fillna(False)
            if show_layoff and 'is_layoff_company' in passed_df.columns:
                mask = mask | passed_df['is_layoff_company'].fillna(False)
            if show_uni and 'is_top_university' in passed_df.columns:
                mask = mask | passed_df['is_top_university'].fillna(False)
            view_df = passed_df[mask]

        st.success(f"**{len(view_df)}** candidates")

        display_cols = ['first_name', 'last_name', 'current_title', 'current_company', 'education', 'location', 'public_url']
        available_cols = [c for c in display_cols if c in view_df.columns]
        st.dataframe(view_df[available_cols], use_container_width=True, hide_index=True,
                    column_config={"public_url": st.column_config.LinkColumn("LinkedIn")})

        csv_data = view_df.to_csv(index=False)
        st.download_button("Download CSV", csv_data, "passed_candidates.csv", "text/csv")

# ========== TAB 4: AI Screening ==========
with tab_screening:
    openai_key = load_openai_key()
    if not openai_key:
        st.warning("OpenAI API key not configured. Add 'openai_api_key' to config.json")
    elif 'results_df' not in st.session_state or st.session_state['results_df'].empty:
        st.info("Load profiles first (Upload tab or PhantomBuster tab), then come back here to screen.")
    else:
        # Use passed_candidates_df if available (filtered), otherwise use results_df (all)
        if 'passed_candidates_df' in st.session_state and not st.session_state['passed_candidates_df'].empty:
            profiles_df = st.session_state['passed_candidates_df']
            st.success(f"**{len(profiles_df)}** filtered candidates ready for screening")
        else:
            profiles_df = st.session_state['results_df']
            st.info(f"**{len(profiles_df)}** profiles loaded (no filters applied)")

        profiles = profiles_df.to_dict('records')

        # Job Description Input
        st.markdown("### Job Description")
        job_description = st.text_area(
            "Paste the job description",
            height=150,
            key="jd_screening",
            placeholder="Paste the full job description here..."
        )

        # Extra Requirements (optional)
        extra_requirements = ""  # Default value
        with st.expander("Extra Requirements (optional)"):
            extra_requirements = st.text_area(
                "Additional must-have criteria",
                height=100,
                key="extra_requirements",
                placeholder="e.g., Must have AWS experience, Hebrew speaker preferred, etc."
            )

        # Screening Configuration
        st.markdown("### Screening Configuration")

        screen_count = st.number_input(
            "Number of profiles to screen",
            min_value=1,
            max_value=len(profiles),
            value=min(100, len(profiles)),
            step=10,
            key="screen_count"
        )

        # Fixed concurrent workers (optimal for Tier 3)
        max_workers = 50

        # Cost estimate
        est_cost = (screen_count * 2500 * 0.15 / 1_000_000) + (screen_count * 400 * 0.60 / 1_000_000)
        est_time = (screen_count / max_workers) * 2  # ~2 seconds per batch
        st.caption(f"Estimated: ~${est_cost:.2f} cost, ~{est_time:.0f} seconds")

        # Debug: Show available fields and test single profile
        with st.expander("Debug: Profile Fields & Test"):
            if profiles:
                st.write("Available fields in profiles:", list(profiles[0].keys()))
                st.write("Sample profile:", {k: str(v)[:100] for k, v in profiles[0].items()})

                if job_description and st.button("Test Single Profile", key="test_single"):
                    try:
                        client = OpenAI(api_key=openai_key)
                        st.write("Testing with first profile...")
                        result = screen_profile(profiles[0], job_description, client, extra_requirements)
                        st.write("Result:", result)
                    except Exception as e:
                        import traceback
                        st.error(f"Error: {e}")
                        st.code(traceback.format_exc())

        # Screen Button
        if job_description:
            if st.button("Start Screening", type="primary", key="start_screening", disabled='screening_in_progress' in st.session_state and st.session_state['screening_in_progress']):
                st.session_state['screening_in_progress'] = True

                profiles_to_screen = profiles[:screen_count]

                # Progress tracking
                status_text = st.empty()
                status_text.info(f"Screening {len(profiles_to_screen)} profiles... This may take a minute.")
                start_time = time.time()

                # Run batch screening (pass API key, not client)
                screening_results = screen_profiles_batch(
                    profiles_to_screen,
                    job_description,
                    openai_key,
                    extra_requirements=extra_requirements if extra_requirements else "",
                    max_workers=max_workers
                )

                # Complete
                elapsed = time.time() - start_time
                status_text.success(f"Completed {len(screening_results)} profiles in {elapsed:.1f}s")

                st.session_state['screening_results'] = screening_results
                st.session_state['screening_in_progress'] = False

                # Send notification
                send_notification("Screening Complete", f"Screened {len(screening_results)} profiles")
                st.rerun()
        else:
            st.warning("Please paste a job description to start screening")

        # Show screening results
        if 'screening_results' in st.session_state and st.session_state['screening_results']:
            st.divider()
            st.markdown("### Screening Results")

            screening_results = st.session_state['screening_results']

            # Summary stats
            stats_col1, stats_col2, stats_col3, stats_col4 = st.columns(4)
            strong_fit = sum(1 for r in screening_results if r.get('fit') == 'Strong Fit')
            good_fit = sum(1 for r in screening_results if r.get('fit') == 'Good Fit')
            partial_fit = sum(1 for r in screening_results if r.get('fit') == 'Partial Fit')
            not_fit = sum(1 for r in screening_results if r.get('fit') == 'Not a Fit')

            stats_col1.metric("Strong Fit", strong_fit, delta=None)
            stats_col2.metric("Good Fit", good_fit, delta=None)
            stats_col3.metric("Partial Fit", partial_fit, delta=None)
            stats_col4.metric("Not a Fit", not_fit, delta=None)

            # Filter by fit level
            fit_filter = st.multiselect(
                "Filter by fit level",
                options=["Strong Fit", "Good Fit", "Partial Fit", "Not a Fit", "Error"],
                default=["Strong Fit", "Good Fit"],
                key="fit_filter"
            )

            # Filter and sort results
            filtered_results = [r for r in screening_results if r.get('fit') in fit_filter]
            sorted_results = sorted(filtered_results, key=lambda x: x.get('score', 0), reverse=True)

            st.info(f"Showing **{len(sorted_results)}** of {len(screening_results)} screened profiles")

            # Create display dataframe
            display_data = []
            for r in sorted_results:
                display_data.append({
                    'Score': r.get('score', 0),
                    'Fit': r.get('fit', ''),
                    'Name': r.get('name', ''),
                    'Title': r.get('current_title', '')[:40],
                    'Company': r.get('current_company', '')[:30],
                    'Summary': r.get('summary', '')[:100],
                    'LinkedIn': r.get('linkedin_url', '')
                })

            df_display = pd.DataFrame(display_data)

            # Display table
            st.dataframe(
                df_display,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "Score": st.column_config.NumberColumn("Score", format="%d/10", width="small"),
                    "Fit": st.column_config.TextColumn("Fit", width="medium"),
                    "Name": st.column_config.TextColumn("Name", width="medium"),
                    "Title": st.column_config.TextColumn("Title", width="medium"),
                    "Company": st.column_config.TextColumn("Company", width="medium"),
                    "Summary": st.column_config.TextColumn("Summary", width="large"),
                    "LinkedIn": st.column_config.LinkColumn("LinkedIn", width="small")
                }
            )

            # Export options
            st.markdown("### Export Results")
            export_col1, export_col2, export_col3 = st.columns(3)

            with export_col1:
                # Full results CSV
                full_df = pd.DataFrame(sorted_results)
                st.download_button(
                    "Download All Results (CSV)",
                    full_df.to_csv(index=False),
                    "screening_results_all.csv",
                    "text/csv"
                )

            with export_col2:
                # Strong + Good fit only
                top_candidates = [r for r in screening_results if r.get('fit') in ['Strong Fit', 'Good Fit']]
                top_df = pd.DataFrame(top_candidates)
                st.download_button(
                    f"Download Top Candidates ({len(top_candidates)})",
                    top_df.to_csv(index=False) if not top_df.empty else "",
                    "screening_results_top.csv",
                    "text/csv",
                    disabled=len(top_candidates) == 0
                )

            with export_col3:
                # Clear results
                if st.button("Clear Results", key="clear_screening"):
                    del st.session_state['screening_results']
                    st.rerun()

# ========== Enrich new profiles (optional) ==========
with st.expander("Enrich New Profiles (requires Crust Data API key)", expanded=False):
    if not has_crust_key:
        st.warning("Crust Data API key not configured. Add 'api_key' to config.json")
    else:
        st.success("Crust Data API key loaded")

        # Source selection tabs
        source_tab1, source_tab2 = st.tabs(["Upload File", "PhantomBuster"])

        urls_to_enrich = []

        with source_tab1:
            uploaded_file = st.file_uploader(
                "Upload CSV or JSON file with LinkedIn URLs",
                type=['csv', 'json'],
                key="enrich_upload"
            )

            if uploaded_file:
                urls = extract_urls(uploaded_file)
                if urls:
                    st.success(f"Found **{len(urls)}** LinkedIn URLs")
                    st.session_state['enrich_urls'] = urls
                    with st.expander("Preview URLs"):
                        for i, url in enumerate(urls[:10]):
                            st.text(f"{i+1}. {url}")
                        if len(urls) > 10:
                            st.text(f"... and {len(urls) - 10} more")
                else:
                    st.warning("No LinkedIn URLs found in file")

        with source_tab2:
            pb_key = load_phantombuster_key()
            has_pb_key = pb_key and pb_key != "YOUR_PHANTOMBUSTER_API_KEY_HERE"

            if not has_pb_key:
                st.warning("PhantomBuster API key not configured. Add 'phantombuster_api_key' to config.json")
            else:
                st.success("PhantomBuster API key loaded")

                # Fetch agents list
                agents = fetch_phantombuster_agents(pb_key)

                if agents:
                    # Create dropdown with agent names
                    agent_options = {a['name']: a['id'] for a in agents}
                    selected_agent = st.selectbox(
                        "Select Phantom",
                        options=list(agent_options.keys()),
                        index=0,
                        help="Select from your PhantomBuster agents"
                    )
                    agent_id = agent_options.get(selected_agent, '')
                else:
                    st.warning("No agents found or could not fetch agent list")
                    agent_id = st.text_input(
                        "PhantomBuster Agent ID",
                        placeholder="Enter your agent/phantom ID manually",
                    )

                if agent_id:
                    if st.button("Fetch from PhantomBuster", key="fetch_pb"):
                        with st.spinner("Fetching results from PhantomBuster..."):
                            pb_df = fetch_phantombuster_result_csv(pb_key, agent_id)

                            if not pb_df.empty:
                                # Normalize column names
                                pb_df = normalize_phantombuster_columns(pb_df)
                                st.success(f"Fetched **{len(pb_df)}** profiles from PhantomBuster")
                                st.session_state['pb_results'] = pb_df

                                # Show preview
                                preview_cols = ['first_name', 'last_name', 'current_title', 'current_company', 'location']
                                available = [c for c in preview_cols if c in pb_df.columns]
                                if available:
                                    st.dataframe(pb_df[available].head(10), use_container_width=True, hide_index=True)

                                # Extract URLs
                                urls = extract_urls_from_phantombuster(pb_df)
                                if urls:
                                    st.info(f"Found **{len(urls)}** LinkedIn URLs to enrich")
                                    st.session_state['enrich_urls'] = urls
                                else:
                                    st.warning("No LinkedIn URLs found in PhantomBuster results")
                            else:
                                st.error("No results found. Check agent ID and make sure agent has run.")

        # Enrichment controls (shared between both sources)
        st.divider()
        if 'enrich_urls' in st.session_state and st.session_state['enrich_urls']:
            urls = st.session_state['enrich_urls']
            st.info(f"**{len(urls)}** URLs ready for enrichment")

            col1, col2 = st.columns(2)
            with col1:
                max_profiles = st.number_input(
                    "Number of profiles to process",
                    min_value=1,
                    max_value=len(urls),
                    value=min(5, len(urls)),
                    help="Start with a few to test"
                )
            with col2:
                batch_size = st.slider("Batch size", min_value=1, max_value=25, value=10)

            if st.button("Start Enrichment", type="primary", key="start_enrich"):
                urls_to_process = urls[:max_profiles]
                results = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                total_batches = (len(urls_to_process) + batch_size - 1) // batch_size

                for i in range(0, len(urls_to_process), batch_size):
                    batch = urls_to_process[i:i + batch_size]
                    batch_num = i // batch_size + 1
                    status_text.text(f"Processing batch {batch_num}/{total_batches}...")
                    batch_results = enrich_batch(batch, api_key)
                    results.extend(batch_results)
                    progress_bar.progress(min((i + batch_size) / len(urls_to_process), 1.0))
                    if i + batch_size < len(urls_to_process):
                        time.sleep(2)

                progress_bar.progress(1.0)
                status_text.text("Enrichment complete!")
                send_notification("Enrichment Complete", f"Processed {len(results)} profiles")
                st.session_state['results'] = results
                st.session_state['results_df'] = flatten_for_csv(results)
                # Clear enrich_urls
                del st.session_state['enrich_urls']
                st.rerun()
